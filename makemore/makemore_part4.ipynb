{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# becoming a backprop ninja\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s: i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "# context length: how many characters do we take to predict the next one?\n",
    "import random\n",
    "block_size = 3\n",
    "\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]  # crop and append\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boilerplate done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {\n",
    "          str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10  # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64  # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)  # for reproducibility\n",
    "C = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * \\\n",
    "    (5/3)/((n_embd * block_size)**0.5)\n",
    "# using b1 just for fun, it's useless because of BN\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))  # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size  # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]  # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3374, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb]  # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1)  # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1  # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "# note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact)  # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2  # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes  # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "# if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "# Yb is the indices of the correct answer. It is going through every row and getting the correct answer index (probability), and getting the mean\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv,  # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "          bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "          embcat, emb]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[1,2], [1,2], [1,2]] = [[3], [3], [3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually,\n",
    "# backpropagating through exactly all of the variables\n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "# -----------------\n",
    "\n",
    "# How to do this: go backwards up and calculate dX for every variable used in reverse order\n",
    "# NOTE: If I want to calculate dX, i'm actually looking at the line below X to see what it is\n",
    "# That makes sense right because that's what actually happening to X\n",
    "# # Forward pass\n",
    "# x = 2\n",
    "# y = x * 3    # y uses x\n",
    "# z = y + 1    # z uses y\n",
    "\n",
    "# # Backward pass\n",
    "# dz = 1       # Starting gradient\n",
    "# dy = dz * 1  # Look at how y was used in z's equation\n",
    "# dx = dy * 3  # Look at how x was used in y's equation\n",
    "\n",
    "# AT EVERY NODE: dL/dSelf = dOut/dSelf * dL/dOut (local derivative * derivative of output)\n",
    "\n",
    "# loss = -logprobs[range(n), Yb].mean():\n",
    "# in `loss = -logprobs[range(n), Yb].mean()` we are doing: loss = -(a+b+c)/3 = -1/3a + -1/3b + -1/3c\n",
    "# the rest are zeros bc not part of the loss, only the Yb plucked out affect it: by dL/dx - 1/n for x in (a,b,c)\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0 / n\n",
    "\n",
    "# logprobs = probs.log():\n",
    "dprobs = (1.0 / probs) * dlogprobs\n",
    "\n",
    "# probs = counts * counts_sum_inv:\n",
    "# ATTENTION: Broadcasting happening bc counts and counts_sum_inv have shape [32,27] and [32,1]\n",
    "# `(counts * dprobs)` is the multiplication, `.sum()` is the replication\n",
    "# a[3x3] * b[3,1] --->\n",
    "# a11*b1 a12*b1 a13*b1      # Same b1 is reused across cols\n",
    "# a21*b2 a22*b2 a23*b2\n",
    "# a31*b3 a32*b3 a33*b3\n",
    "\n",
    "# Micrograd: if thing is used multiple times, must sum\n",
    "\n",
    "# Each element in counts_sum_inv was \"copied\" 27 times during broadcasting\n",
    "# This is a general rule: when backpropagating through broadcasted operations, you need to sum across the broadcasted dimensions to get the correct gradient.\n",
    "# rule: if the variable is broadcasted, need this\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "\n",
    "# probs = counts * counts_sum_inv\n",
    "dcounts = (counts_sum_inv) * dprobs\n",
    "\n",
    "# counts_sum_inv = counts_sum**-1\n",
    "dcounts_sum = (-1 * counts_sum ** -2) * dcounts_sum_inv\n",
    "\n",
    "# Looking at `counts_sum = counts.sum(1, keepdims=True)`: counts [32,7] and counts_sum [32,1]\n",
    "# Let counts = a, count_sum = b. Then how do the b's depend on a's? That is local derivative.\n",
    "# a11 a12 a13  --->   b1 (= a11 + a12 + a13)   -- b1 only depends on the 1st row, b2 on 2nd, etc.\n",
    "# a21 a22 a23  --->   b2 (= a21 + a22 + a23)\n",
    "# a31 a32 a33  --->   b3 (= a31 + a32 + a33)\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "\n",
    "# counts = norm_logits.exp()\n",
    "dnorm_logits = counts * dcounts\n",
    "\n",
    "# norm_logits = logits - logit_maxes: [32,27] - [32,1]\n",
    "# c11 c12 c13 = a11 a12 a13     b1\n",
    "# c21 c22 c23 = a21 a22 a23  -  b2\n",
    "# c31 c32 c33 = a31 a32 a33     b3\n",
    "\n",
    "# so c32 = a32 - b3\n",
    "# - means that the c's flow to each of the a's but also to the b's but b's are broadcast so need sum\n",
    "dlogits = dnorm_logits.clone()  # flow\n",
    "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
    "\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# pluck out the maxes\n",
    "dlogits += F.one_hot(logits.max(1).indices,\n",
    "                     num_classes=logits.shape[1]) * dlogit_maxes\n",
    "\n",
    "# onwards just watched\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0)\n",
    "dhpreact = (1.0 - h**2) * dh\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "dbnraw = bngain * dhpreact\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "dbndiff += (2*bndiff) * dbndiff2\n",
    "dhprebn = dbndiff.clone()\n",
    "dbnmeani = (-dbndiff).sum(0)\n",
    "dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0)\n",
    "demb = dembcat.view(emb.shape)\n",
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k, j]\n",
    "        dC[ix] += demb[k, j]\n",
    "\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.337357759475708 diff: 2.384185791015625e-07\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 7.2177499532699585e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass (derive by doing math and simplifying the operation of the softmax)\n",
    "\n",
    "# convert logits into probs w/ each row summing to 1 (this is model's current preds)\n",
    "dlogits = F.softmax(logits, 1)\n",
    "\n",
    "# gradient says \"decrease probability for wrong predictions, increase for right ones\"\n",
    "# - If model predicted 0.7 for correct class, gradient is -0.3 (0.7 - 1)\n",
    "# - If model predicted 0.3 for wrong class, gradient is 0.3 (0.3 - 0)\n",
    "dlogits[range(n), Yb] -= 1\n",
    "dlogits /= n\n",
    "\n",
    "# I can only get approximate to be true, my maxdiff is 6e-9\n",
    "cmp('logits', dlogits, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Think about what we want the model to do:**\n",
    "\n",
    "For CORRECT predictions (where Yb points):\n",
    "\n",
    "- If model says 0.7 probability (too low, should be 1.0)\n",
    "- Gradient = 0.7 - 1 = -0.3\n",
    "- Negative gradient means \"increase this logit\"\n",
    "- Because when we update weights with `-learning_rate * gradient`, it becomes a positive change\n",
    "\n",
    "For WRONG predictions:\n",
    "\n",
    "- If model says 0.3 probability (too high, should be 0.0)\n",
    "- Gradient = 0.3 - 0 = 0.3\n",
    "- Positive gradient means \"decrease this logit\"\n",
    "- Because when we update weights with -learning_rate \\* gradient, it becomes a negative change\n",
    "- So subtracting 1 from the true class creates exactly the push/pull we want:\n",
    "\n",
    "So subtracting 1 from the true class creates exactly the push/pull we want:\n",
    "**Pull UP** probabilities for correct answers (negative gradient)\n",
    "**Push DOWN** probabilities for wrong answers (positive gradient)\n",
    "The subtraction is what gives us this opposite behavior for correct vs incorrect predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, Yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0684, 0.0845, 0.0188, 0.0499, 0.0189, 0.0830, 0.0230, 0.0349, 0.0179,\n",
       "        0.0319, 0.0352, 0.0362, 0.0373, 0.0287, 0.0365, 0.0148, 0.0087, 0.0198,\n",
       "        0.0165, 0.0567, 0.0527, 0.0217, 0.0239, 0.0685, 0.0615, 0.0274, 0.0227],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(logits, 1)[0]\n",
    "# Look at prob distribution of the first sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0684,  0.0845,  0.0188,  0.0499,  0.0189,  0.0830,  0.0230,  0.0349,\n",
       "        -0.9821,  0.0319,  0.0352,  0.0362,  0.0373,  0.0287,  0.0365,  0.0148,\n",
       "         0.0087,  0.0198,  0.0165,  0.0567,  0.0527,  0.0217,  0.0239,  0.0685,\n",
       "         0.0615,  0.0274,  0.0227], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0] * n\n",
    "# Rescale the gradient back to original magnitude before it was averaged over the batch size\n",
    "# See how the one correct sample is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0955e-09, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0].sum()  # this sums to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount of push and pull is exactly equalized because they sum to zero. Why do they sum to 0? Because we just took the softmax of the logits which normally makes them sum to 1 because they are probabilities. But instead we just subtracted 1 from the correct index.\n",
    "\n",
    "**The amount that your solution is incorrect is the exact amount your logits will be pushed and pulled:** For instance if it perfectly predicted output, then the `dlogits` would be all 0s and the gradient would effectuate no change.\n",
    "\n",
    "### This is the magic of cross entropy loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14a7eaa80>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(dlogits.detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(0.0062, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / \\\n",
    "    torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.8012\n",
      "  10000/ 200000: 2.2046\n",
      "  20000/ 200000: 2.3818\n",
      "  30000/ 200000: 2.4745\n",
      "  40000/ 200000: 1.9834\n",
      "  50000/ 200000: 2.3524\n",
      "  60000/ 200000: 2.4452\n",
      "  70000/ 200000: 2.0964\n",
      "  80000/ 200000: 2.3501\n",
      "  90000/ 200000: 2.1230\n",
      " 100000/ 200000: 2.0386\n",
      " 110000/ 200000: 2.3359\n",
      " 120000/ 200000: 2.0256\n",
      " 130000/ 200000: 2.3642\n",
      " 140000/ 200000: 2.3013\n",
      " 150000/ 200000: 2.2053\n",
      " 160000/ 200000: 1.9383\n",
      " 170000/ 200000: 1.8414\n",
      " 180000/ 200000: 2.0360\n",
      " 190000/ 200000: 1.9603\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10  # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200  # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)  # for reproducibility\n",
    "C = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * \\\n",
    "    (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))  # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size  # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "with torch.no_grad():\n",
    "\n",
    "    # kick off optimization\n",
    "    for i in range(max_steps):\n",
    "\n",
    "        # minibatch construct\n",
    "        ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "        Xb, Yb = Xtr[ix], Ytr[ix]  # batch X,Y\n",
    "\n",
    "        # forward pass\n",
    "        emb = C[Xb]  # embed the characters into vectors\n",
    "        embcat = emb.view(emb.shape[0], -1)  # concatenate the vectors\n",
    "        # Linear layer\n",
    "        hprebn = embcat @ W1 + b1  # hidden layer pre-activation\n",
    "        # BatchNorm layer\n",
    "        # -------------------------------------------------------------\n",
    "        bnmean = hprebn.mean(0, keepdim=True)\n",
    "        bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "        bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "        bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "        hpreact = bngain * bnraw + bnbias\n",
    "        # -------------------------------------------------------------\n",
    "        # Non-linearity\n",
    "        h = torch.tanh(hpreact)  # hidden layer\n",
    "        logits = h @ W2 + b2  # output layer\n",
    "        loss = F.cross_entropy(logits, Yb)  # loss function\n",
    "\n",
    "        # backward pass\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        # loss.backward() # use this for correctness comparisons, delete it later!\n",
    "\n",
    "        # manual backprop! #swole_doge_meme\n",
    "        # -----------------\n",
    "        dlogits = F.softmax(logits, 1)\n",
    "        dlogits[range(n), Yb] -= 1\n",
    "        dlogits /= n\n",
    "        # 2nd layer backprop\n",
    "        dh = dlogits @ W2.T\n",
    "        dW2 = h.T @ dlogits\n",
    "        db2 = dlogits.sum(0)\n",
    "        # tanh\n",
    "        dhpreact = (1.0 - h**2) * dh\n",
    "        # batchnorm backprop\n",
    "        dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "        dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "        dhprebn = bngain*bnvar_inv/n * \\\n",
    "            (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "        # 1st layer\n",
    "        dembcat = dhprebn @ W1.T\n",
    "        dW1 = embcat.T @ dhprebn\n",
    "        db1 = dhprebn.sum(0)\n",
    "        # embedding\n",
    "        demb = dembcat.view(emb.shape)\n",
    "        dC = torch.zeros_like(C)\n",
    "        for k in range(Xb.shape[0]):\n",
    "            for j in range(Xb.shape[1]):\n",
    "                ix = Xb[k, j]\n",
    "                dC[ix] += demb[k, j]\n",
    "        grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "        # -----------------\n",
    "\n",
    "        # update\n",
    "        lr = 0.1 if i < 100000 else 0.01  # step learning rate decay\n",
    "        for p, grad in zip(parameters, grads):\n",
    "            # p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "            p.data += -lr * grad  # new way of swole doge TODO: enable\n",
    "\n",
    "        # track stats\n",
    "        if i % 10000 == 0:  # print every once in a while\n",
    "            print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "        lossi.append(loss.log10().item())\n",
    "\n",
    "        # if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "    # pass the training set through\n",
    "    emb = C[Xtr]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    # measure the mean/std over the entire training set\n",
    "    bnmean = hpreact.mean(0, keepdim=True)\n",
    "    bnvar = hpreact.var(0, keepdim=True, unbiased=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.071840763092041\n",
      "val 2.1087331771850586\n"
     ]
    }
   ],
   "source": [
    "# evaluate train and val loss\n",
    "\n",
    "@torch.no_grad()  # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "    x, y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte),\n",
    "    }[split]\n",
    "    emb = C[x]  # (N, block_size, n_embd)\n",
    "    embcat = emb.view(emb.shape[0], -1)  # concat into (N, block_size * n_embd)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "    h = torch.tanh(hpreact)  # (N, n_hidden)\n",
    "    logits = h @ W2 + b2  # (N, vocab_size)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carmah.\n",
      "amori.\n",
      "kif.\n",
      "jari.\n",
      "reetlanna.\n",
      "sane.\n",
      "mahubefaresynci.\n",
      "aqui.\n",
      "ner.\n",
      "kia.\n",
      "chaiiv.\n",
      "kaleigh.\n",
      "ham.\n",
      "joce.\n",
      "quint.\n",
      "salin.\n",
      "alianni.\n",
      "waythoniearisia.\n",
      "kaellinsley.\n",
      "dae.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "\n",
    "    out = []\n",
    "    context = [0] * block_size  # initialize with all ...\n",
    "    while True:\n",
    "        # ------------\n",
    "        # forward pass:\n",
    "        # Embedding\n",
    "        emb = C[torch.tensor([context])]  # (1,block_size,d)\n",
    "        # concat into (N, block_size * n_embd)\n",
    "        embcat = emb.view(emb.shape[0], -1)\n",
    "        hpreact = embcat @ W1 + b1\n",
    "        hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "        h = torch.tanh(hpreact)  # (N, n_hidden)\n",
    "        logits = h @ W2 + b2  # (N, vocab_size)\n",
    "        # ------------\n",
    "        # Sample\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

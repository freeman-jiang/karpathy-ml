{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"names.txt\") as f:\n",
    "  words = f.read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itos={1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# build vocab and mappings to/from ints\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(f\"{itos=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "X, Y = [], []\n",
    "for w in words[:5]:\n",
    "  \n",
    "  # print(w)\n",
    "  context = [0] * block_size\n",
    "  for ch in w + '.':\n",
    "    ix = stoi[ch]\n",
    "    X.append(context)\n",
    "    Y.append(ix)\n",
    "    # print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "    context = context[1:] + [ix] # crop and append (rolling window)\n",
    "  \n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((27, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((6,100)) # 6 inputs, 100 outputs (hidden layer size or hyperparameter)\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = emb.view(32, 6) @ W1 + b1 # hidden layer\n",
    "h.shape\n",
    "# ^ 100 dimensional activations for each of the training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # hidden layer, tanh makes it -1 to 1\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting check: This is correct because the same 100D bias vector is added to each of the 32 training examples\n",
    "# 32, 100\n",
    "#   , 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100, 27)) # input 100 from hidden layer, output 27 (one for each character)\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = h @ W2 + b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = logits.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = counts / counts.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4813e-05, 4.9507e-12, 3.7101e-04, 4.4213e-05, 1.4031e-06, 4.0345e-01,\n",
       "         3.6777e-08, 2.8899e-10, 2.3362e-11, 1.4314e-02, 1.3185e-13, 1.1037e-06,\n",
       "         1.2677e-03, 5.4338e-03, 4.2434e-06, 3.3549e-06, 1.7512e-10, 2.3876e-06,\n",
       "         5.7134e-01, 5.9527e-09, 2.6583e-07, 5.4498e-07, 7.7641e-07, 3.7423e-04,\n",
       "         1.7179e-09, 3.3473e-03, 3.4533e-05],\n",
       "        [9.8116e-11, 1.3091e-09, 9.7746e-12, 8.5157e-04, 2.4377e-12, 1.3418e-04,\n",
       "         1.1531e-05, 9.2460e-09, 3.3055e-13, 9.9606e-09, 3.9576e-11, 1.7197e-02,\n",
       "         1.2075e-08, 5.4115e-06, 3.8283e-06, 4.5141e-08, 8.6070e-03, 4.1409e-08,\n",
       "         1.6570e-09, 1.0854e-09, 9.5931e-01, 2.2277e-12, 1.3808e-02, 6.1580e-07,\n",
       "         8.9950e-13, 4.5421e-06, 7.1229e-05],\n",
       "        [1.3723e-03, 1.6301e-10, 2.2812e-05, 3.6982e-06, 3.7219e-02, 4.0616e-04,\n",
       "         1.1141e-01, 3.9589e-11, 3.8617e-02, 7.7586e-07, 5.0776e-14, 8.2956e-07,\n",
       "         7.9574e-01, 1.7263e-10, 1.2285e-05, 1.8921e-06, 2.1999e-12, 1.7806e-05,\n",
       "         9.4828e-06, 1.3831e-03, 1.8078e-07, 2.8242e-04, 1.3351e-02, 1.4220e-04,\n",
       "         1.4691e-10, 2.7435e-08, 8.8677e-06],\n",
       "        [3.6218e-07, 5.0775e-11, 6.3889e-07, 3.5224e-08, 2.3061e-05, 5.7112e-05,\n",
       "         4.0084e-09, 7.7235e-14, 4.5289e-07, 6.3658e-03, 4.7503e-11, 6.9202e-08,\n",
       "         5.9353e-05, 4.0802e-04, 4.9137e-05, 3.8735e-04, 5.4629e-13, 3.2276e-06,\n",
       "         9.9031e-01, 3.2206e-14, 5.3002e-05, 6.2987e-08, 1.1511e-08, 4.1253e-13,\n",
       "         2.6838e-11, 2.2831e-03, 7.8656e-10],\n",
       "        [9.1275e-09, 4.3511e-09, 2.7697e-03, 1.5713e-04, 1.1790e-08, 2.3546e-02,\n",
       "         1.0172e-09, 8.0404e-07, 2.4911e-09, 1.1013e-01, 8.1710e-11, 6.6915e-04,\n",
       "         1.8720e-04, 3.4551e-02, 1.4327e-05, 3.2588e-06, 1.7995e-11, 5.2173e-05,\n",
       "         9.5926e-02, 1.7875e-08, 1.2877e-06, 1.0173e-06, 4.3060e-05, 6.7276e-01,\n",
       "         3.0216e-09, 5.7880e-02, 1.3067e-03],\n",
       "        [1.4813e-05, 4.9507e-12, 3.7101e-04, 4.4213e-05, 1.4031e-06, 4.0345e-01,\n",
       "         3.6777e-08, 2.8899e-10, 2.3362e-11, 1.4314e-02, 1.3185e-13, 1.1037e-06,\n",
       "         1.2677e-03, 5.4338e-03, 4.2434e-06, 3.3549e-06, 1.7512e-10, 2.3876e-06,\n",
       "         5.7134e-01, 5.9527e-09, 2.6583e-07, 5.4498e-07, 7.7641e-07, 3.7423e-04,\n",
       "         1.7179e-09, 3.3473e-03, 3.4533e-05],\n",
       "        [8.2097e-13, 1.7858e-11, 6.8732e-10, 2.2735e-04, 2.3468e-13, 1.1554e-03,\n",
       "         1.0442e-07, 3.0241e-09, 3.0948e-08, 1.5278e-07, 1.8979e-15, 9.7811e-01,\n",
       "         3.1971e-05, 1.1029e-06, 4.0519e-08, 3.8334e-06, 1.3607e-12, 2.9623e-07,\n",
       "         1.3347e-08, 1.4906e-11, 1.0766e-07, 4.5042e-15, 1.3755e-04, 2.6328e-06,\n",
       "         6.9532e-14, 9.4148e-06, 2.0321e-02],\n",
       "        [1.1611e-07, 2.2400e-11, 3.7428e-04, 5.8991e-08, 6.6531e-03, 1.3929e-08,\n",
       "         8.1626e-07, 2.3327e-11, 8.8315e-01, 2.0841e-04, 6.9806e-14, 6.3050e-11,\n",
       "         4.4425e-02, 4.1515e-15, 2.6545e-07, 8.1620e-10, 2.5020e-13, 1.7854e-09,\n",
       "         5.2927e-08, 3.2496e-06, 2.5094e-11, 6.5168e-02, 1.0484e-07, 1.4452e-05,\n",
       "         3.6653e-09, 1.6489e-10, 4.4154e-06],\n",
       "        [5.3284e-11, 1.1010e-05, 5.8503e-17, 2.2842e-06, 4.5227e-13, 7.8872e-07,\n",
       "         2.4292e-11, 4.5631e-08, 4.2540e-19, 2.5246e-08, 6.7522e-11, 2.3157e-09,\n",
       "         2.5139e-12, 1.6392e-01, 9.5716e-09, 3.6249e-11, 3.7931e-08, 2.6588e-14,\n",
       "         3.2885e-09, 4.5226e-16, 8.3606e-01, 3.1520e-11, 5.0416e-06, 3.1833e-07,\n",
       "         2.1873e-11, 4.0963e-11, 2.3452e-11],\n",
       "        [2.0749e-10, 5.1902e-12, 3.0488e-09, 1.5022e-09, 1.1366e-05, 5.2962e-04,\n",
       "         7.8660e-05, 1.5434e-11, 1.1997e-03, 2.4476e-12, 1.6302e-12, 4.1029e-05,\n",
       "         9.8100e-01, 5.0058e-11, 3.4147e-09, 4.8670e-03, 1.7963e-18, 2.3761e-08,\n",
       "         8.4223e-09, 1.7990e-05, 9.2443e-12, 4.9908e-14, 1.2246e-02, 9.1201e-07,\n",
       "         2.2499e-11, 1.2907e-10, 2.8676e-06],\n",
       "        [3.7685e-05, 2.7518e-04, 2.9180e-08, 4.3271e-01, 2.2779e-10, 3.2036e-07,\n",
       "         2.2791e-08, 2.0161e-05, 1.5437e-09, 1.1849e-01, 5.2146e-08, 1.3719e-05,\n",
       "         5.6713e-06, 3.5103e-03, 3.4686e-04, 1.6511e-06, 3.5352e-02, 2.0254e-09,\n",
       "         2.9437e-04, 1.5468e-10, 3.9168e-01, 1.7099e-02, 1.3368e-04, 2.6516e-08,\n",
       "         8.3348e-08, 2.6763e-07, 3.0853e-05],\n",
       "        [1.6607e-12, 2.9343e-12, 2.1858e-15, 1.2100e-09, 7.7871e-10, 1.7430e-09,\n",
       "         9.9215e-11, 1.7581e-05, 3.7551e-13, 9.5192e-11, 1.3422e-11, 2.5495e-08,\n",
       "         1.9869e-10, 3.1128e-07, 1.0110e-12, 1.4784e-10, 1.4416e-12, 8.6070e-13,\n",
       "         4.9338e-12, 1.1546e-12, 2.3177e-10, 9.4060e-12, 3.8865e-05, 9.9994e-01,\n",
       "         7.3743e-08, 6.7172e-14, 5.6033e-11],\n",
       "        [1.4813e-05, 4.9507e-12, 3.7101e-04, 4.4213e-05, 1.4031e-06, 4.0345e-01,\n",
       "         3.6777e-08, 2.8899e-10, 2.3362e-11, 1.4314e-02, 1.3185e-13, 1.1037e-06,\n",
       "         1.2677e-03, 5.4338e-03, 4.2434e-06, 3.3549e-06, 1.7512e-10, 2.3876e-06,\n",
       "         5.7134e-01, 5.9527e-09, 2.6583e-07, 5.4498e-07, 7.7641e-07, 3.7423e-04,\n",
       "         1.7179e-09, 3.3473e-03, 3.4533e-05],\n",
       "        [3.0153e-09, 5.5873e-11, 1.3108e-05, 7.4318e-04, 3.2817e-08, 5.2443e-01,\n",
       "         1.5930e-05, 1.6250e-08, 1.6770e-08, 3.9844e-03, 3.0082e-14, 2.6940e-01,\n",
       "         4.7752e-03, 1.0400e-03, 4.5435e-06, 5.2546e-05, 9.0391e-11, 1.9138e-04,\n",
       "         2.1342e-03, 1.7648e-08, 1.5324e-07, 2.4951e-09, 4.7417e-04, 2.3994e-04,\n",
       "         7.6011e-11, 1.9970e-02, 1.7253e-01],\n",
       "        [2.3183e-09, 3.2499e-15, 5.6104e-05, 9.8748e-08, 5.4733e-06, 1.1508e-04,\n",
       "         3.5006e-08, 5.8472e-13, 2.6445e-03, 3.4875e-07, 2.4616e-13, 9.0369e-06,\n",
       "         9.9695e-01, 1.9412e-11, 1.8317e-10, 3.9712e-05, 3.3289e-20, 6.7654e-08,\n",
       "         2.2765e-08, 1.7382e-04, 4.1936e-13, 4.9752e-10, 1.1228e-06, 1.3788e-06,\n",
       "         9.4172e-12, 5.9609e-08, 1.1673e-08],\n",
       "        [3.8346e-08, 5.2193e-07, 4.7695e-04, 8.4467e-03, 7.9721e-07, 9.3004e-08,\n",
       "         3.7267e-10, 9.1257e-07, 1.0853e-11, 9.6109e-01, 1.2371e-10, 2.0709e-06,\n",
       "         1.3970e-05, 3.2884e-03, 1.0098e-04, 3.1069e-06, 3.2026e-12, 1.3123e-07,\n",
       "         1.9714e-02, 3.2673e-12, 1.3831e-04, 3.5152e-04, 2.1013e-06, 2.8026e-03,\n",
       "         4.7817e-08, 3.5574e-03, 7.3390e-06],\n",
       "        [1.4813e-05, 4.9507e-12, 3.7101e-04, 4.4213e-05, 1.4031e-06, 4.0345e-01,\n",
       "         3.6777e-08, 2.8899e-10, 2.3362e-11, 1.4314e-02, 1.3185e-13, 1.1037e-06,\n",
       "         1.2677e-03, 5.4338e-03, 4.2434e-06, 3.3549e-06, 1.7512e-10, 2.3876e-06,\n",
       "         5.7134e-01, 5.9527e-09, 2.6583e-07, 5.4498e-07, 7.7641e-07, 3.7423e-04,\n",
       "         1.7179e-09, 3.3473e-03, 3.4533e-05],\n",
       "        [1.0633e-09, 1.1755e-08, 2.1221e-09, 1.6863e-02, 1.1493e-10, 1.7180e-03,\n",
       "         1.3893e-04, 1.8917e-07, 7.6577e-12, 3.6813e-07, 2.6894e-11, 4.2614e-01,\n",
       "         9.3488e-07, 2.0059e-04, 8.7298e-05, 1.5258e-06, 1.1363e-03, 1.5294e-06,\n",
       "         1.2518e-06, 1.1149e-08, 5.1394e-01, 1.8446e-11, 3.0050e-02, 1.4569e-05,\n",
       "         4.9320e-11, 2.5354e-04, 9.4492e-03],\n",
       "        [9.3189e-01, 8.4075e-10, 3.4970e-05, 5.6133e-06, 4.0734e-02, 7.6719e-05,\n",
       "         5.6942e-05, 4.2071e-11, 1.5403e-05, 7.0142e-06, 3.5689e-14, 9.7958e-09,\n",
       "         2.2311e-03, 1.9901e-08, 7.8381e-06, 2.6112e-07, 1.3081e-10, 7.8200e-08,\n",
       "         5.3364e-04, 2.3876e-07, 2.6369e-05, 3.7142e-03, 5.8446e-05, 2.0603e-02,\n",
       "         1.1180e-09, 7.7792e-08, 3.5523e-06],\n",
       "        [2.0755e-11, 1.5066e-09, 2.1631e-12, 4.6649e-06, 3.7004e-11, 4.5604e-03,\n",
       "         6.9892e-08, 3.5972e-10, 9.4790e-13, 4.0554e-08, 2.9003e-13, 1.8924e-05,\n",
       "         1.2475e-05, 7.8725e-01, 1.8326e-07, 5.2173e-06, 5.5414e-11, 1.1495e-11,\n",
       "         1.9251e-01, 3.9252e-17, 1.4574e-05, 1.0916e-11, 7.6460e-03, 2.6074e-12,\n",
       "         4.6037e-11, 7.9315e-03, 4.9773e-05],\n",
       "        [4.3510e-06, 1.0065e-08, 5.0076e-06, 2.1815e-03, 4.2385e-07, 3.7346e-06,\n",
       "         2.2089e-04, 2.5645e-07, 3.1628e-04, 8.5058e-03, 4.3193e-12, 4.3109e-04,\n",
       "         4.1450e-04, 1.0521e-07, 1.1577e-04, 1.2312e-09, 2.2660e-06, 3.5898e-05,\n",
       "         6.6808e-06, 8.1289e-06, 4.2750e-03, 9.0620e-08, 2.1482e-04, 9.3596e-01,\n",
       "         2.7941e-08, 6.6205e-07, 4.7300e-02],\n",
       "        [1.3095e-08, 3.0957e-04, 4.5105e-15, 4.6236e-06, 3.3793e-11, 2.6504e-05,\n",
       "         3.2951e-09, 3.6547e-07, 1.6132e-15, 1.0614e-07, 3.8085e-08, 9.3041e-07,\n",
       "         2.0490e-08, 3.9468e-04, 4.9649e-07, 1.5907e-10, 4.8465e-04, 6.1372e-14,\n",
       "         4.9774e-12, 2.3539e-10, 9.5595e-01, 1.4489e-08, 2.3488e-04, 4.2594e-02,\n",
       "         6.8857e-07, 6.8064e-14, 7.2606e-08],\n",
       "        [2.7269e-05, 1.9863e-06, 6.6816e-10, 5.9701e-02, 9.8730e-02, 8.3476e-04,\n",
       "         2.2856e-03, 1.4187e-08, 2.5257e-04, 4.0543e-06, 5.7203e-10, 1.6661e-04,\n",
       "         7.7135e-01, 1.0850e-08, 2.8099e-04, 5.2428e-05, 1.1553e-08, 4.1912e-05,\n",
       "         4.5212e-07, 4.1553e-04, 1.2860e-03, 3.7866e-04, 4.1747e-02, 2.2432e-02,\n",
       "         2.9914e-06, 2.4728e-08, 8.5479e-06],\n",
       "        [4.1122e-07, 2.4604e-07, 1.9427e-08, 1.2008e-03, 8.5305e-05, 1.1877e-02,\n",
       "         3.0857e-04, 3.4551e-11, 6.7339e-03, 7.1591e-01, 8.9714e-08, 4.5727e-06,\n",
       "         3.7294e-02, 2.5161e-03, 2.6906e-02, 3.3466e-03, 3.4177e-08, 6.2756e-05,\n",
       "         1.4976e-01, 3.2118e-10, 2.9253e-03, 4.6058e-05, 2.0618e-03, 9.7940e-10,\n",
       "         2.7062e-07, 3.8875e-02, 8.8446e-05],\n",
       "        [2.1541e-08, 2.6491e-07, 7.6131e-06, 4.6956e-03, 2.3872e-07, 4.3506e-01,\n",
       "         4.2345e-07, 6.8928e-07, 2.8725e-09, 7.4609e-03, 1.6839e-10, 1.3172e-03,\n",
       "         1.5326e-03, 2.5806e-02, 5.1246e-05, 5.0578e-04, 1.6856e-11, 2.7921e-07,\n",
       "         7.6853e-02, 2.5365e-10, 1.1672e-03, 2.1228e-07, 2.5500e-03, 4.2239e-01,\n",
       "         2.8388e-08, 1.5803e-02, 4.7947e-03],\n",
       "        [1.4813e-05, 4.9507e-12, 3.7101e-04, 4.4213e-05, 1.4031e-06, 4.0345e-01,\n",
       "         3.6777e-08, 2.8899e-10, 2.3362e-11, 1.4314e-02, 1.3185e-13, 1.1037e-06,\n",
       "         1.2677e-03, 5.4338e-03, 4.2434e-06, 3.3549e-06, 1.7512e-10, 2.3876e-06,\n",
       "         5.7134e-01, 5.9527e-09, 2.6583e-07, 5.4498e-07, 7.7641e-07, 3.7423e-04,\n",
       "         1.7179e-09, 3.3473e-03, 3.4533e-05],\n",
       "        [5.3020e-05, 4.3342e-14, 2.8035e-03, 6.7468e-07, 2.3982e-07, 7.8940e-01,\n",
       "         9.7016e-11, 5.6429e-12, 1.0548e-10, 1.1093e-03, 1.1252e-14, 1.1970e-08,\n",
       "         2.6729e-04, 3.2445e-04, 4.5337e-07, 9.8909e-07, 1.8516e-12, 1.1480e-07,\n",
       "         2.0601e-01, 6.5629e-09, 1.1555e-08, 4.3350e-07, 1.3781e-09, 1.0245e-06,\n",
       "         7.1215e-11, 2.7969e-05, 4.3804e-08],\n",
       "        [5.7088e-14, 6.1529e-13, 6.2875e-10, 4.6612e-05, 5.0424e-15, 6.1857e-04,\n",
       "         5.9649e-10, 3.4177e-10, 1.0723e-10, 1.9021e-07, 1.7328e-15, 9.9848e-01,\n",
       "         1.5701e-06, 6.1783e-06, 7.4195e-09, 3.2804e-07, 2.2079e-13, 5.1820e-08,\n",
       "         1.9689e-08, 9.2661e-13, 1.5682e-09, 1.9956e-15, 2.8044e-06, 2.7659e-08,\n",
       "         6.8722e-15, 5.4935e-05, 7.9242e-04],\n",
       "        [4.2736e-18, 1.6303e-11, 1.4230e-09, 2.7111e-08, 8.9949e-12, 1.8710e-13,\n",
       "         4.4415e-08, 1.3925e-08, 9.4318e-01, 2.9179e-07, 8.9238e-11, 7.0531e-07,\n",
       "         3.1744e-04, 1.3585e-17, 2.7486e-13, 4.1589e-12, 7.2058e-17, 1.6157e-11,\n",
       "         4.8710e-19, 1.7511e-07, 4.1424e-14, 3.0093e-13, 7.0041e-10, 4.3751e-06,\n",
       "         7.4803e-10, 4.4672e-13, 5.6498e-02],\n",
       "        [9.2756e-09, 6.0241e-04, 5.3201e-05, 2.0394e-09, 6.2166e-01, 6.7078e-11,\n",
       "         1.2750e-06, 5.8107e-05, 2.6665e-01, 9.4595e-04, 2.4799e-03, 1.1663e-09,\n",
       "         2.0405e-02, 9.3024e-11, 2.5770e-08, 2.3244e-04, 1.3627e-14, 1.8569e-07,\n",
       "         1.0257e-09, 1.9515e-09, 7.8193e-09, 2.6400e-05, 1.2229e-08, 4.9146e-06,\n",
       "         8.6881e-02, 1.7457e-11, 7.9454e-07],\n",
       "        [1.1587e-12, 5.4572e-06, 1.2222e-11, 3.2855e-05, 5.3261e-11, 3.4875e-09,\n",
       "         1.0824e-10, 2.8605e-02, 2.2922e-16, 7.9256e-04, 8.2612e-08, 2.5809e-09,\n",
       "         4.5798e-15, 8.2877e-01, 1.5731e-06, 1.2474e-09, 9.2889e-09, 7.7578e-10,\n",
       "         2.3527e-08, 2.2018e-16, 1.4173e-01, 1.3239e-08, 6.6036e-10, 6.3771e-05,\n",
       "         1.3157e-08, 7.2873e-10, 6.5657e-13],\n",
       "        [4.9512e-13, 2.2721e-12, 6.7297e-16, 1.7274e-10, 3.0825e-10, 5.1493e-10,\n",
       "         3.2228e-11, 1.8727e-05, 1.6166e-13, 7.0312e-11, 1.3572e-11, 4.6494e-09,\n",
       "         4.6339e-11, 1.6122e-07, 1.8751e-13, 8.5015e-11, 3.7674e-13, 4.4283e-13,\n",
       "         8.9573e-13, 3.8568e-13, 3.4057e-11, 2.7071e-12, 2.4904e-06, 9.9998e-01,\n",
       "         9.3794e-08, 7.8251e-15, 3.5622e-12]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob # the probability of each character for each training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.0345e-01, 5.4115e-06, 1.7263e-10, 5.0775e-11, 9.1275e-09, 3.3549e-06,\n",
       "        3.1971e-05, 2.0841e-04, 5.0416e-06, 2.4476e-12, 2.7518e-04, 1.6607e-12,\n",
       "        4.9507e-12, 4.7417e-04, 3.2499e-15, 3.8346e-08, 1.4314e-02, 1.1149e-08,\n",
       "        8.4075e-10, 2.1631e-12, 3.7346e-06, 2.0490e-08, 7.7135e-01, 2.4604e-07,\n",
       "        2.1541e-08, 5.9527e-09, 9.8909e-07, 2.2079e-13, 9.4318e-01, 9.4595e-04,\n",
       "        5.4572e-06, 4.9512e-13])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[torch.arange(32), Y] # given the label, the probability of the model assigns to the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.9552)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -prob[torch.arange(32), Y].log().mean() # negative log likelihood loss\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
